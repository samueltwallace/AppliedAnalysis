\documentclass[../main.tex]{subfiles}

\addbibresource{../bibs/lsc-variational.bib}

\begin{document}
\chapter{Lower---Semicontinuity and Material Science}

\emph{Contributed by Samuel Wallace\footnote{contact: \href{mailto:swalla24@uic.edu}{swalla24@uic.edu}}} \\

Materials are most precisely characterized by their atomic arrangements.
If you had perfectly accurate data on the positions and arrangements of atoms at any one particular time and if you had a very powerful computer, you could go ahead and calculate absolutely everything about an object of that material: electrical and magnetic properties, mechanical properties, and thermodynamic quantities.
However, such a powerful computer and that data is far out of reach currently.
Therefore to figure out properties of materials, we need a more feasible model. \\

A well---studied model for materials is a continuum model.
In this model, we assume a material is made of continuous ``stuff'' that entirely fills the space the object takes up, and this ``stuff'' has a more--or-less continuously distributed properties.
Mathematically, the object is seen as an open set $\Omega \subset \mathbb{R}^n$, and various functions are defined on this set to represent the various properties. \\

In this section, I'll be analyzing some mechanical properties of elastic objects through the continuum modeling lens.

\section{Mathematical Modeling of Elastostatics and the Direct Method}

Elastic materials are those that deform when forces are applied, and return to their initial state after the forces are removed.
A model for elastic materials is the \emph{hyper---elastic} model: given a map $y: \Omega \to \mathbb{R}^3$ that represents the position and orientation of our material sample in space, we have a function $W: \mathrm{Mat}_{n\times n} \to \mathbb{R}$ so that $W(\nabla y(x))$ is the energy density of the deformation at a point $x$.
We may then integrate this function over $\Omega$ to get the total energy of our deformation. \\

A seemingly simple problem to analyze is how elastic objects deform when they are not moving; these are \emph{elastostatic} problems.
The key modeling assumption is that these are global or local minimizers of the energy.
The idea behind this assumption is that derivative of energy is forces, and if there were forces on this object it would move; so the derivative of energy must be zero. \\

This explanation is heuristic, but can be justified, to various degrees, mathematically.
For now, we will leave it as an assumption, but keep the result: our hope is to understand the quantity

\begin{equation}
  \label{eq:min-energy}
  \min_{y: \Omega \to \mathbb{R}^n} E(y) = \min_{y: \Omega \to \mathbb{R}^n} \int_{\Omega}W( \nabla y) dx
\end{equation}

under some assumptions on $y$; for example, we could impose continuity or regularity assumptions, or boundary conditions, or applied forces, and so on.
For our discussion now, we won't worry about which kind of restrictions we put on $y$, but we will make more abstract assumptions on the restrictions in relation to the outcome of the problem \eqref{eq:min-energy}.
For example, we could ask that our restrictions give us a unique solution. \\

How could we verify that an elastostatics problem has a minimizer? To do so, we'll introduce the Direct Method in the Calculus of Variations.
This is a proof technique for showing existence of a global (i.e. over all allowed functions) minimizer  of a variational problem. \\

The basic strategy to to the direct method is to take any sequence of functions whose energy approaches the minimum, show it converges in some relevant topology, and show the limit actually gets the minimum value.
The convergence is usually gotten by choosing a coarser topology on the relevant functions, and the last step requires, at least, lower--semicontinuity of the integrand; for us, $W$.
To see an example of its application, let's take a look at the classic direct method proof from \cite{evans_partial_2010}.
We will take $n=1$ for simplicity, i.e. $y: \Omega \to \mathbb{R}$.
Before that, we need a small, but significant result:

\begin{lem}
  If $W$ is a convex function on the set of matrices, then it is lower semi--continuous on $W^{1,p}(\Omega)$ in its weak topology for $1 < p < \infty$, where $W^{1,p}$ is a Sobolev space (cf. \ref{apdx:sobolev}).
\end{lem}

\begin{proof}
  Let $y_k \rightharpoonup y$ in $W^{1,p}(\Omega)$; we wish to show that $\liminf_{k \to \infty} E(y_k) \geq E(y)$.
  Then it follows from Soboelv inequalities and Rellich--Kondrachov that $y_k \rightharpoonup y$ strongly in $L^p$, and their gradients converge weakly in $L^p$. 
  Because $W$ is convex, we have the pointwise inequality $W(\nabla y_k) - W(\nabla y) \geq  DW(\nabla y) \cdot (\nabla y_{k} - \nabla y)$, so that integrating,
  \begin{equation*}
    \int_{\Omega} W(\nabla y_k) \geq \int_{\Omega} W(\nabla y) dx + \int_{\Omega} DW ( \nabla y) \cdot (\nabla y_k - \nabla y) dx
  \end{equation*}

  And the last term goes to zero as $\nabla y_{k} \rightharpoonup \nabla y$ in $L^p$.
\end{proof}

\begin{thm}
  Assume the following about the problem \eqref{eq:min-energy}:
  \begin{itemize}
  \item $W(F) \geq \alpha \abs{F}^p - \beta$ for some constant $\alpha, \beta > 0$ and $1 < p < \infty$ (coercivity)
  \item $W$ is a convex function, (convexity)
  \item The set of admissible functions are non--empty and sequentially closed in the weak topology on $W^{1,p}(\Omega)$. (weak closure)
  \end{itemize}

  Then the problem admits a map $y$ that takes on the minimum value; i.e. the minimum is achieved by a specific map.
  In addition, if $W$ is strictly convex, then the minimizer is unique.
\end{thm}

\begin{proof}
  First note that $E(y) \geq - \beta \abs{\Omega}$, so that the minimum is not $- \infty$.
  Now choose a sequence of maps $y_k$ so that $E(y_k) \to \min E(y) = \mathcal{E}$.
  Immediately from the first assumption on the problem, $E(y_k) \geq \int_{\Omega} \abs{\nabla y_k}^{p} dx - \beta$, so we then conclude $y_k$ is bounded in $W^{1,p}(\Omega)$, and Banach--Alaoglu, weakly precompact.
  Thus we may extract a convergent subsequence, not relabeled, so that $y_k \to y$ strongly in $L^p(\Omega)$.
  Since the admissible functions are weakly closed, the limit $y$ is also admissible, and since $W$ is convex and hence weakly lower--semicontinuous, we have that
  \begin{equation*}
    \mathcal{E} = \liminf_{k \to \infty} E(y_k) \geq E(y) \geq \mathcal{E}
  \end{equation*}

  So $E(y) = \mathcal{E}$. \\

  To see uniqueness, assume $W$ is strictly convex.
  Then suppose there $E(y_1) = E(y_2) = \mathcal{E}$.
  Then $E((y_1+y_2)/2) < (E(y_1) + E(y_2))/2 = \mathcal{E}$, which contradicts the minimality of $E$ unless $y_1 = y_2$.
\end{proof}

Now we can see in a little more detail why the assumptions were necessary for the direct method, and what the strategy is for other problems:

\begin{enumerate}
\item Coercivity gives us norm bounds on the minimizing sequence in a function space, which allow us to transer to functional analytic statements about the sequence.
\item We then pass to a compact subspace of our original space to claim convergence of a subsequence.
\item Weak closure then says the limit of this convergent subsequence is valid for the problem we are addressing.
\item Convexity implies lower--semicontinuity, which says our convergent subsequence actually gets to the minimum value without jumping up in a discontinuous way.
\end{enumerate}


All of these are important for the direct method to work, and can be modified to a variety of problems.
However, not all of these assumptions are totally necessary for a minimizer to exist.
In particular, convexity is a rather strong assumption to impose to conclude lower--semicontinuity.
This isn't clear from our setup; in fact one can prove that for real--valued functions, convexity and lower--semicontinuity are equivalent.
But we shouldn't be considering real--valued functions, we should be considering suitable maps $y: \Omega \to \mathbb{R}^n$ for $\Omega \subset \mathbb{R}^n$ and $n \geq 2$ for 3--dimensional objects!
And as we will see, convexity is not necessary, and in application, is too much to ask for.

\section{Other Types of Convexity}

Here we will explore other assumptions on $W$ that give lower--semicontinuity, but are not exactly our usual definition of convexity.
These will be polyconvexity, quasiconvexity, and rank--one convexity.
A natural question to ask is why these conditions are all thought of convexity.
Our usual convexity is defined by an inequality on line segment in our relevant space: $W(tx + (1-t)y) \leq t W(x) + (1-t) W(y)$ for $t \in [0,1]$.
Our exotic convexity conditions will have the same inequality, but the allowed segments in our condition will vary. \\

Let's first give the canonical definition and relations of each form of convexity; these come from \cite{dacorogna_direct_2008}.
Since we are interested in our material science problems, we are thinking of $W$ as a function on $\nabla y$, so we will consider $W$ a function on $n \times n$ matrices, $\mathbb{R}^{n^2}$.


\begin{defn}[Polyconvexity]
  $W$ is polyconvex if it can be written as a convex function of the determinants of the minors $M_{i_1, \ldots, i_k, j_1, \ldots, j_k}$ (the matrix formed by deleting the $i_1, \ldots, i_k$ columns and the $j_1, \ldots, j_k$ rows, $k < n$.
  In a formula,
  \begin{equation*}
    W(M) = F( \set{\mathrm{det} M_{i_1, \ldots, i_k, j_i, \ldots, j_k}} )
  \end{equation*}

  Where $F$ is convex in its arguments.
\end{defn}

This is a generalization of convexity, but a little mysterious.
Why determinants of minors submatrices?
What makes them so special?

\begin{defn}[Quasiconvexity]
  $W$ is quasiconvex if for every $F \in \mathbb{R}^{n^2}$,
  \begin{equation*}
    w(F) \leq \frac{1}{\abs{U}} \int_U W(F + \nabla \varphi) dx
  \end{equation*}

  For all functions $\varphi \in W^{1,\infty}_0(U; \mathbb{R}^n)$ and all open bounded sets $U \subset \mathbb{R}^n$ (i.e. Lipschitz functions that vanish at the boundary of $U$).
\end{defn}

This is even more mysterious.
There is no apparent relation to convexity in this definition. \\

Finally,

\begin{defn}[Rank--one Convexity]
  $W$ is rank--one convex if for all matrices $F$ and $G$ with $\mathrm{rank}(F-G) \leq 1$, we have the convexity inequality:
  \begin{equation*}
    W(t F + (1-t) G) \leq t W(F) + (1-t) W(G)
  \end{equation*}

  For all $t \in [0,1]$.
\end{defn}

This at least looks like convexity, with a definiton incorporating values along segments; although the rank--one condition is still mysterious. \\

Putting the mystery of the definitions to the side, we will relate our various notions of convexity, and some implications.

\begin{lem}
  The determinant function is rank--one convex and quasiconvex.
  Moreover, determinants are rank--one affine (i.e. equality holds for all $t$ in the definition) and quasiaffine (i.e. equality holds in the definition).
\end{lem}

\begin{proof}
  We work in the 2$\times$2 case and the rest follow by induction. \\

  Let $F$ be a two--by--two matrix, with entries $f_{ij}$.
  Then if $\mathrm{rank}(F - \eta) \leq 1$, we can write $\eta = F + a \otimes b$ for vectors $a,b \in \mathbb{R}^n$.
  Then
  
\begin{align*}
  \mathrm{det} ( \lambda F + (1- \lambda) \eta) &= \mathrm{det} ( F + (1-\lambda) a \otimes b) \\
                                 &= f_{11} f_{22} - f_{12}f_{21} + (1-\lambda) (f_{11} a_2b_2 + f_{22} a_1b_1 = f_{12} a_2b_1 - f_{21} a_1b_2) \\
  &= \lambda \mathrm{det}(F) + (1-\lambda) \mathrm{det} (\eta)
\end{align*}

Thus $\mathrm{det}$ is rank-one affine. \\

For quasiconvexity, note that for any function $\varphi \in C_0^2( U; \mathbb{R}^2)$,

\begin{equation*}
  \mathrm{det} ( \nabla \varphi) = \partial_1\varphi_1 \cdot \partial_2\varphi_2 - \partial_2\varphi_1 \cdot \partial_1 \varphi_2 = \partial_1 ( \varphi_1 \partial_2 \varphi_2) - \partial_2 ( \varphi_1 \partial_1 \varphi_2)
\end{equation*}

Where $\partial_i = \frac{\partial}{\partial x_i}$ and the $\varphi_i$ the components of the values of $\phi$.
Then when we integrate this, we may integrate by parts to get

\begin{equation*}
  \int_U f_{11} \partial_2 \varphi_2 + f_{22} \partial_1 \varphi_1 - f_{12} \partial_1 \varphi_2 - f_{21} \partial_2 \varphi_1 + \mathrm{det} \nabla \varphi dx
\end{equation*}

adding $\left| D \right| \mathrm{det} (F)$ to both sides and rearranging, we get quasiconvexity.

\end{proof}


\begin{thm}
  $W$ is convex $\Rightarrow$ $W$ is polyconvex $\Rightarrow$ $W$ is quasiconvex $\Rightarrow$ $W$ is rank--one convex.
\end{thm}

\begin{proof}

  
\begin{itemize}
\item[($\Rightarrow$)]   $W$ convex certain implies polyconvex; we may use the one--by--one minor submatrices (i.e. the elements of the matrix) and then by varying each of these, convexity on $W$ implies convexity of our function of entries. 
\item[($\Rightarrow$)] Now suppose $W$ is polyconvex.
  Then

  \begin{align*}
    \frac{1}{\abs{U}} \int_U W(M + \nabla \varphi) &= \frac{1}{\abs{U}} \int_U F( \set{\mathrm{det}( M + \nabla \varphi)_{ij}} ) dx \\
                                     &\geq F\left( \frac{1}{\abs{U}} \int_U \mathrm{det} (M + \nabla \varphi)_{ij} dx \right) \\
    
                                     &= F \left( \int_U \mathrm{det} (M) dx \right) = W(M) \\
  \end{align*}

  Where the inequality was gotten by Jensen's inequality.

\end{itemize}


  
\end{proof}


\printbibliography

\end{document}